{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> DROWSINESS DETECTION </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import random,shutil\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout,Conv2D,Flatten,Dense, MaxPooling2D, BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                  shear_range = 0.2,\n",
    "                                  zoom_range=0.2,\n",
    "                                  horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Loading Data Set</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2467 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set=train_datagen.flow_from_directory('yawn_eye/dataset_new/train',\n",
    "                                              target_size=(24,24),\n",
    "                                              batch_size=32,\n",
    "                                              class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 433 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory('yawn_eye/dataset_new/test',\n",
    "                                           target_size=(24,24),\n",
    "                                           batch_size=32,\n",
    "                                           class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Building Model using CNN</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Conv2D(filters=32, kernel_size=(3,3),padding=\"same\",activation=\"relu\", input_shape=[24,24,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(MaxPooling2D(pool_size=1,strides=2,padding=\"valid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Conv2D(filters=32, kernel_size=(3,3),padding=\"same\",activation=\"relu\"))\n",
    "cnn.add(MaxPooling2D(pool_size=1,strides=2,padding=\"valid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Dense(units=4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "78/78 [==============================] - 213s 3s/step - loss: 0.7856 - accuracy: 0.6526 - val_loss: 0.5502 - val_accuracy: 0.6790\n",
      "Epoch 2/15\n",
      "78/78 [==============================] - 11s 145ms/step - loss: 0.4896 - accuracy: 0.7726 - val_loss: 0.4283 - val_accuracy: 0.7621\n",
      "Epoch 3/15\n",
      "78/78 [==============================] - 13s 161ms/step - loss: 0.4221 - accuracy: 0.7925 - val_loss: 0.3090 - val_accuracy: 0.7968\n",
      "Epoch 4/15\n",
      "78/78 [==============================] - 12s 155ms/step - loss: 0.3917 - accuracy: 0.8058 - val_loss: 0.3212 - val_accuracy: 0.8083\n",
      "Epoch 5/15\n",
      "78/78 [==============================] - 13s 161ms/step - loss: 0.3615 - accuracy: 0.8164 - val_loss: 0.2508 - val_accuracy: 0.8291\n",
      "Epoch 6/15\n",
      "78/78 [==============================] - 12s 159ms/step - loss: 0.3440 - accuracy: 0.8253 - val_loss: 0.2727 - val_accuracy: 0.8245\n",
      "Epoch 7/15\n",
      "78/78 [==============================] - 12s 154ms/step - loss: 0.3363 - accuracy: 0.8249 - val_loss: 0.2686 - val_accuracy: 0.7945\n",
      "Epoch 8/15\n",
      "78/78 [==============================] - 12s 154ms/step - loss: 0.3354 - accuracy: 0.8289 - val_loss: 0.4135 - val_accuracy: 0.8037\n",
      "Epoch 9/15\n",
      "78/78 [==============================] - 12s 159ms/step - loss: 0.3029 - accuracy: 0.8484 - val_loss: 0.4817 - val_accuracy: 0.8060\n",
      "Epoch 10/15\n",
      "78/78 [==============================] - 12s 158ms/step - loss: 0.3177 - accuracy: 0.8375 - val_loss: 0.3691 - val_accuracy: 0.8268\n",
      "Epoch 11/15\n",
      "78/78 [==============================] - 13s 173ms/step - loss: 0.2982 - accuracy: 0.8512 - val_loss: 0.2532 - val_accuracy: 0.8337\n",
      "Epoch 12/15\n",
      "78/78 [==============================] - 13s 162ms/step - loss: 0.3029 - accuracy: 0.8431 - val_loss: 0.7035 - val_accuracy: 0.8199\n",
      "Epoch 13/15\n",
      "78/78 [==============================] - 11s 141ms/step - loss: 0.3071 - accuracy: 0.8375 - val_loss: 0.4097 - val_accuracy: 0.8268\n",
      "Epoch 14/15\n",
      "78/78 [==============================] - 11s 143ms/step - loss: 0.2829 - accuracy: 0.8662 - val_loss: 0.3946 - val_accuracy: 0.8476\n",
      "Epoch 15/15\n",
      "78/78 [==============================] - 11s 142ms/step - loss: 0.2621 - accuracy: 0.8650 - val_loss: 0.2617 - val_accuracy: 0.8152\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x14bb5528188>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit_generator(training_set,\n",
    "                 steps_per_epoch = 78,\n",
    "                 epochs=15,\n",
    "                 validation_data=test_set,\n",
    "                 validation_steps=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model\n",
    "cnn.save('CNN_DD.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Closed': 0, 'Open': 1, 'no_yawn': 2, 'yawn': 3}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Testing the model by uploading an image</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yawn'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_image = image.load_img('yawn_eye/dataset_new/test/yawn/286.jpg', target_size = (24,24))\n",
    "plt.imshow(test_image)\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)\n",
    "print(result)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'Closed'\n",
    "elif result[0][1] == 1:\n",
    "    prediction = 'Open'\n",
    "elif result[0][2] == 1:\n",
    "    prediction = 'no yawn'\n",
    "else:\n",
    "    prediction = 'yawn'\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF2.0]",
   "language": "python",
   "name": "conda-env-py3-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
